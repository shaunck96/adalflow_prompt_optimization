# -*- coding: utf-8 -*-
"""adalflow_object_count_auto_optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n3mHUWekTEYHiBdYBTw43TKlPN41A9za

# 🤗 Welcome to AdalFlow!
## The PyTorch library to auto-optimize any LLM task pipelines

Thanks for trying us out, we're here to provide you with the best LLM application development experience you can dream of 😊 any questions or concerns you may have, [come talk to us on discord,](https://discord.gg/ezzszrRZvT) we're always here to help!


# Quick Links

Github repo: https://github.com/SylphAI-Inc/AdalFlow

Full Tutorials: https://adalflow.sylph.ai/index.html#.

Deep dive on each API: check out the [developer notes](https://adalflow.sylph.ai/tutorials/index.html).

Common use cases along with the auto-optimization:  check out [Use cases](https://adalflow.sylph.ai/use_cases/index.html).

# Outline

*Note: As training can consume tokens fast, and the notebook runtime will reset everytime you use, it might be better for you to learn training in your local editor.*

This is a quick introduction of AdalFlow on question answering use case end to end

* Trainable Task pipeline with trainable parameters
* Create AdalComponent for your task pipeline
* Use Trainer to diagnose, debug, and to train.

You can find all source code here: https://github.com/SylphAI-Inc/AdalFlow/tree/main/use_cases/question_answering/bhh_object_count

**Here is the more detailed tutorial for the code here: https://adalflow.sylph.ai/use_cases/question_answering.html**


# Installation

1. Use `pip` to install the `adalflow` Python package. We will need `openai`, `groq`, and `faiss`(cpu version) from the extra packages.

  ```bash
  pip install adalflow[openai,groq,faiss-cpu]
  ```
2. Setup  `openai` and `groq` API key in the environment variables
"""

from IPython.display import clear_output

!pip install -U adalflow[openai,groq,datasets]

clear_output()

import adalflow as adal

adal.__version__

"""## Set Environment Variables

Run the following code and pass your api key.

Note: for normal `.py` projects, follow our [official installation guide](https://lightrag.sylph.ai/get_started/installation.html).

*Go to [OpenAI](https://platform.openai.com/docs/introduction) and [Groq](https://console.groq.com/docs/) to get API keys if you don't already have.*
"""

import os

from getpass import getpass

# Prompt user to enter their API keys securely
openai_api_key = getpass("Please enter your OpenAI API key: ")
groq_api_key = getpass("Please enter your GROQ API key: ")


# Set environment variables
os.environ['OPENAI_API_KEY'] = openai_api_key
os.environ['GROQ_API_KEY'] = groq_api_key

print("API keys have been set.")

"""

# 😇 Trainable Task Pipeline

We will create a task pipeline consists of a generator, with a customzied template, a customized output parser.

Different from our other pipelines where the `prompt_kwargs` values are strings, but here we will use ``Parameter``. And we will set up two parameter, one is of ``ParameterType.PROMPT`` and the other of type ``ParameterType.DEMOS``. The first one will be trained by text-grad and the second will be trained by boostrap few shot optimizer.


"""

import adalflow as adal
import re
from typing import Dict, Union
import adalflow as adal
from adalflow.optim.types import ParameterType


@adal.fun_to_component
def parse_integer_answer(answer: str):
    """A function that parses the last integer from a string using regular expressions."""
    try:
        # Use regular expression to find all sequences of digits
        numbers = re.findall(r"\d+", answer)
        if numbers:
            # Get the last number found
            answer = int(numbers[-1])
        else:
            answer = -1
    except ValueError:
        answer = -1

    return answer


few_shot_template = r"""<START_OF_SYSTEM_PROMPT>
{{system_prompt}}
{# Few shot demos #}
{% if few_shot_demos is not none %}
Here are some examples:
{{few_shot_demos}}
{% endif %}
<END_OF_SYSTEM_PROMPT>
<START_OF_USER>
{{input_str}}
<END_OF_USER>
"""

class ObjectCountTaskPipeline(adal.Component):
    def __init__(self, model_client: adal.ModelClient, model_kwargs: Dict):
        super().__init__()

        system_prompt = adal.Parameter(
            data="You will answer a reasoning question. Think step by step. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value.",
            role_desc="To give task instruction to the language model in the system prompt",
            requires_opt=True,
            param_type=ParameterType.PROMPT,
        )
        few_shot_demos = adal.Parameter(
            data=None,
            role_desc="To provide few shot demos to the language model",
            requires_opt=True,  # Changed to True for few-shot learning
            param_type=ParameterType.DEMOS,
        )

        self.llm_counter = adal.Generator(
            model_client=model_client,
            model_kwargs=model_kwargs,
            template=few_shot_template,
            prompt_kwargs={
                "system_prompt": system_prompt,
                "few_shot_demos": few_shot_demos,
            },
            output_processors=parse_integer_answer,
            use_cache=True,
        )

    def call(
        self, question: str, id: str = None
    ) -> Union[adal.GeneratorOutput, adal.Parameter]:
        output = self.llm_counter(prompt_kwargs={"input_str": question}, id=id)
        return output

"""Next, we will run this pipeline in both train and eval mode.

#### Eval mode with GeneratorOutput

Eval mode will output ``GeneratorOutput``.

#### Train mode with different form of output

Train mode will return ``Parameter``, where the `data` field will be the `raw_response`` from the GeneratorOutput, and we put the full GeneratorOutput at the ``full_response`` in the parameter.

As the `data` field of the `Parameter` directly communicate with the Optimizer, which are an LLM itself, its better than they understand exactly the string response itself instead of the parsed one.

Later you will see that we also use ``eval_input`` of the parameter to communicate with the `LossFunction` as that need the parsed final output.
"""

from adalflow.components.model_client.openai_client import OpenAIClient
from adalflow.components.model_client.groq_client import GroqAPIClient


gpt_3_model = {
    "model_client": OpenAIClient(),
    "model_kwargs": {
        "model": "gpt-3.5-turbo",
        "max_tokens": 2000,
        "temperature": 0.0,
        "top_p": 0.99,
        "frequency_penalty": 0,
        "presence_penalty": 0,
        "stop": None,
    },
}

llama_3_1_model ={
    "model_client": GroqAPIClient(),
    "model_kwargs": {
        "model": "llama-3.1-8b-instant"
    }
}

gpt_4o_model = {
    "model_client": OpenAIClient(),
    "model_kwargs": {
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 0.99,
        "frequency_penalty": 0,
        "presence_penalty": 0,
        "stop": None,
    },
}


question = "I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a trumpet. How many musical instruments do I have?"
task_pipeline = ObjectCountTaskPipeline(**gpt_3_model)
print(task_pipeline)

answer = task_pipeline(question, id="1")
print(answer)

# set it to train mode
task_pipeline.train()
answer = task_pipeline(question, id="1")
print(answer)
print(f"full_response: {answer.full_response}")

!pip install datasets
clear_output()

"""### Load Datasets"""

from adalflow.datasets.big_bench_hard import BigBenchHard
from adalflow.utils.data import subset_dataset

def load_datasets(max_samples: int = None):
    """Load the dataset"""
    train_data = BigBenchHard(split="train")
    val_data = BigBenchHard(split="val")
    test_data = BigBenchHard(split="test")

    # Limit the number of samples
    if max_samples:
        train_data = subset_dataset(train_data, max_samples)
        val_data = subset_dataset(val_data, max_samples)
        test_data = subset_dataset(test_data, max_samples)

    return train_data, val_data, test_data

# check the datasets

train_data, val_data, test_data = load_datasets(max_samples=2)
print(train_data[0])

"""### Soft link to AdalFlow default file path

Lets' match the default to the current project, so that you can see the downloaded data and later the checkpoints of the training.
"""

! ln -s /root/.adalflow /content/adalflow

# go to files then you will see a folder named as adalflow

"""# 😊 AdalComponent to define everything we need to train

1. We need `backward_engine_model_config`` for ``backward_engine`` to compute gradient.

2. We need ``text_optimizer_model_config`` for the `text optimizer` for propose new prompts.

3. For the demo optimizer, we need a `teacher_model_config` to config a teacher generator, in this case, it is the `llm_counter`. The teacher will share the same prompt with the `llm_counter` but you can use a more advanced model.

In general, we should have all of these parts to use a more advanced model.

## 🧑 Diagnose

Diagnose is more of an evaluation, but with detailed logs so that you can manually inspect the wrong output.

This one shows the minimum config you need to get the `diagnose` work.
"""

from adalflow.datasets.types import Example
from adalflow.eval.answer_match_acc import AnswerMatchAcc


class ObjectCountAdalComponent(adal.AdalComponent):
    def __init__(self, model_client: adal.ModelClient, model_kwargs: Dict):
        task = ObjectCountTaskPipeline(model_client, model_kwargs)
        eval_fn = AnswerMatchAcc(type="exact_match").compute_single_item
        super().__init__(task=task, eval_fn=eval_fn)

    def handle_one_task_sample(self, sample: Example):
        return self.task.call, {"question": sample.question, "id": sample.id}

    def evaluate_one_sample(
        self, sample: Example, y_pred: adal.GeneratorOutput
    ) -> float:
        y_label = -1
        if y_pred and y_pred.data:
            y_label = y_pred.data
        return self.eval_fn(y_label, sample.answer)

def diagnose(
    model_client: adal.ModelClient,
    model_kwargs: Dict,
) -> Dict:

    trainset, valset, testset = load_datasets()
    # use max_samples=10 to test the code

    adal_component = ObjectCountAdalComponent(model_client, model_kwargs)
    trainer = adal.Trainer(adaltask=adal_component)
    trainer.diagnose(dataset=trainset, split="train")
    trainer.diagnose(dataset=valset, split="val")
    trainer.diagnose(dataset=testset, split="test")

diagnose(**gpt_3_model)

"""Now, you can go to `/content/adalflow/ckpt/ObjectCountAdalComponent/diagnose_train/stats.json` to view the average score for each split. And also the `diagnose.json` for different errors.

Here is the overall score for each split.

| Train  | Val| Test |
|:--------- |:--------:| ---------:|
| 0.88      | 0.8   |    0.83  |

## 🐛 Debug

## ✅ Train

Now, let's start training.
"""

from adalflow.datasets.types import Example
from adalflow.eval.answer_match_acc import AnswerMatchAcc


class ObjectCountAdalComponent(adal.AdalComponent):
    def __init__(
        self,
        model_client: adal.ModelClient,
        model_kwargs: Dict,
        backward_engine_model_config: Dict,
        teacher_model_config: Dict,
        text_optimizer_model_config: Dict,
    ):
        task = ObjectCountTaskPipeline(model_client, model_kwargs)
        eval_fn = AnswerMatchAcc(type="exact_match").compute_single_item
        loss_fn = adal.EvalFnToTextLoss(
            eval_fn=eval_fn,
            eval_fn_desc="exact_match: 1 if str(y) == str(y_gt) else 0",
        )
        super().__init__(task=task, eval_fn=eval_fn, loss_fn=loss_fn)

        self.backward_engine_model_config = backward_engine_model_config
        self.teacher_model_config = teacher_model_config
        self.text_optimizer_model_config = text_optimizer_model_config

    def handle_one_task_sample(self, sample: Example):
        return self.task.call, {"question": sample.question, "id": sample.id}

    def evaluate_one_sample(
        self, sample: Example, y_pred: adal.GeneratorOutput
    ) -> float:
        y_label = -1
        if y_pred and y_pred.data:
            y_label = y_pred.data
        return self.eval_fn(y=y_label, y_gt=sample.answer)

    def handle_one_loss_sample(self, sample: Example, pred: adal.Parameter):
        # prepare gt parameter
        y_gt = adal.Parameter(
            name="y_gt",
            data=sample.answer,
            eval_input=sample.answer,
            requires_opt=False,
        )

        # pred's full_response is the output of the task pipeline which is GeneratorOutput
        pred.eval_input = pred.full_response.data
        return self.loss_fn, {"kwargs": {"y": pred, "y_gt": y_gt}}

    def configure_backward_engine(self):
        super().configure_backward_engine_helper(
            **self.backward_engine_model_config
        )

    def configure_teacher_generator(self):
        super().configure_teacher_generator_helper(
            **self.teacher_model_config
        )

    def configure_optimizers(self):
        to = super().configure_text_optimizer_helper(**self.text_optimizer_model_config)
        do = super().configure_demo_optimizer_helper()  # Add demo optimizer
        return to + do  # Return both text and demo optimizers

def train(
    train_batch_size=4,  # larger batch size is not that effective, probably because of llm's lost in the middle
    raw_shots: int = 0,
    bootstrap_shots: int = 1,
    max_steps=1,
    num_workers=4,
    strategy="random",
    optimization_order="sequential",
    debug=False,
    resume_from_ckpt=None,
    exclude_input_fields_from_bootstrap_demos=False,
):
    adal_component = ObjectCountAdalComponent(
        **gpt_3_model,
        teacher_model_config=gpt_4o_model,
        text_optimizer_model_config=gpt_4o_model,
        backward_engine_model_config=gpt_4o_model
    )
    print(adal_component)
    trainer = adal.Trainer(
        train_batch_size=train_batch_size,
        adaltask=adal_component,
        strategy=strategy,
        max_steps=max_steps,
        num_workers=num_workers,
        raw_shots=raw_shots,
        bootstrap_shots=bootstrap_shots,
        debug=debug,
        weighted_sampling=True,
        optimization_order=optimization_order,
        exclude_input_fields_from_bootstrap_demos=exclude_input_fields_from_bootstrap_demos,
    )
    print(trainer)

    train_dataset, val_dataset, test_dataset = load_datasets()
    trainer.fit(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        test_dataset=test_dataset,
        debug=debug,
        resume_from_ckpt=resume_from_ckpt,
    )

"""We use `Sequential` in default, we will end up with 24 steps in total, 12 for text optimizer and 12 for the demo optimizer."""

train(debug=False, max_steps=12, strategy="constrained",
      raw_shots=0, bootstrap_shots=1,
      exclude_input_fields_from_bootstrap_demos=True
      )

"""Here is our scores for each step:

"val_scores": [
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.84,
        0.84,
        0.84,
        0.84,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86,
        0.86
    ]

  "test_scores": [
        0.83,
        0.83,
        0.83,
        0.83,
        0.83,
        0.91,
        0.91,
        0.91,
        0.91,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87,
        0.87
    ]


It is normal when the score of the validation does not exactly match to that of the test set. You can also train with just the test set. You can modify the fit arguments as

```
trainer.fit(
        train_dataset=train_dataset,
        val_dataset=test_dataset,
        # test_dataset=test_dataset,
        debug=debug,
        resume_from_ckpt=resume_from_ckpt,
    )
```

# 🔥 Resume Checkpoint

We might want to continue from the earlier step and to train more steps

This is easy to do.

**Note: Ensure you copy the path you had, and replace it, as your run might create a different file name.**
"""

ckpt_path = "/content/adalflow/ckpt/ObjectCountAdalComponent/constrained_max_steps_12_4e8a1_run_1.json"

train(debug=False, max_steps=12, strategy="constrained",
                raw_shots=0, bootstrap_shots=1,
                resume_from_ckpt=ckpt_path,
                exclude_input_fields_from_bootstrap_demos=True)

"""I decide to try more, this time, using strategy "random". And in the bootstrap demo, there is one shot, but I ensure I also add the "input" in the demonstration."""

train(debug=False, max_steps=12, strategy="random",
                raw_shots=0, bootstrap_shots=1,
                resume_from_ckpt=ckpt_path,
                exclude_input_fields_from_bootstrap_demos=False)

"""Finally, we got 96% on the val and 95% on the test!!! This is really close to GPT4o's performance. This took us 72 steps!

The score is consistent, meaning this is a good prompt.
Here is our final optimized prompt:

System:

```

"prompt": [
                {
                    "id": "327b63f0-b532-435a-85d7-6137d4e52c4c",
                    "name": "llm_counter.system_prompt",
                    "data": "You will answer a reasoning question. Carefully count each item and verify your total. List each item individually and ensure accuracy. Show your calculations step by step. The last line of your response should be: 'Answer: $VALUE' where VALUE is a numerical value.",
                    "requires_opt": true
                },
                {
                    "id": "73a3953b-6351-44d8-a36f-7521db346cca",
                    "name": "llm_counter.few_shot_demos",
                    "data": "input_str: I have a yam, a cauliflower, a bed, two cabbages, a garlic, an oven, a\n  carrot, a head of broccoli, a potato, a stalk of celery, a lettuce head, and a toaster.\n  How many vegetables do I have?\nExample: 'Let''s list and count each vegetable individually:\n\n\n  1. Yam\n\n  2. Cauliflower\n\n  3. Cabbage (1)\n\n  4. Cabbage (2)\n\n  5. Garlic\n\n  6. Carrot\n\n  7. Broccoli\n\n  8. Potato\n\n  9. Celery\n\n  10. Lettuce\n\n\n  Now, let''s verify the count:\n\n\n  1. Yam\n\n  2. Cauliflower\n\n  3. Cabbage (1)\n\n  4. Cabbage (2)\n\n  5. Garlic\n\n  6. Carrot\n\n  7. Broccoli\n\n  8. Potato\n\n  9. Celery\n\n  10. Lettuce\n\n\n  Total number of vegetables: 10\n\n\n  Answer: 10'",
                    "requires_opt": true
                }
            ]
```


You will see all steps record from the log.

Happy Optimizing!!!

# Issues and feedback

If you encounter any issues, please report them here: [GitHub Issues](https://github.com/SylphAI-Inc/LightRAG/issues).

For feedback, you can use either the [GitHub discussions](https://github.com/SylphAI-Inc/LightRAG/discussions) or [Discord](https://discord.gg/ezzszrRZvT).
"""